{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import os\n",
    "import random\n",
    "\n",
    "# Set the seed for reproducibility\n",
    "random.seed(42)\n",
    "# folder_name=\"train\"\n",
    "type = \"all-3\"\n",
    "base_path = f\"../disfluency-spotify-wikipedia/wikipedia-model-files\"\n",
    "zero_txt = base_path + f\"/{type}/0.txt\"\n",
    "one_text = base_path + f\"/{type}/1.txt\"\n",
    "two_text = base_path + f\"/{type}/2.txt\"\n",
    "three_text = base_path + f\"/{type}/3.txt\"\n",
    "four_text = base_path + f\"/{type}/4.txt\"\n",
    "five_text = base_path + f\"/{type}/5.txt\"\n",
    "six_text = base_path + f\"/{type}/6.txt\"\n",
    "seven_text = base_path + f\"/{type}/7.txt\"\n",
    "eight_text = base_path + f\"/{type}/8.txt\"\n",
    "nine_text = base_path + f\"/{type}/9.txt\"\n",
    "ten_text = base_path + f\"/{type}/10.txt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "zero_texts = open(zero_txt).read().split('\\n')\n",
    "one_texts = open(one_text).read().split('\\n')\n",
    "two_texts = open(two_text).read().split('\\n')\n",
    "three_texts = open(three_text).read().split('\\n')\n",
    "four_texts = open(four_text).read().split('\\n')\n",
    "five_texts = open(five_text).read().split('\\n')\n",
    "six_texts = open(six_text).read().split('\\n')\n",
    "seven_texts = open(seven_text).read().split('\\n')\n",
    "eight_texts = open(eight_text).read().split('\\n')\n",
    "nine_texts = open(nine_text).read().split('\\n')\n",
    "ten_texts = open(ten_text).read().split('\\n')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5001\n",
      "ERROR \n",
      "ERROR \n",
      "ERROR \n",
      "ERROR \n",
      "ERROR \n",
      "ERROR \n",
      "ERROR \n",
      "ERROR \n",
      "ERROR \n",
      "ERROR \n",
      "ERROR \n",
      "5000\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Ensure that both lists are of equal length\n",
    "assert len(zero_texts) == len(one_texts) == len(two_texts) == len(three_texts) == len(four_texts) == len(five_texts) == len(six_texts) == len(seven_texts) == len(eight_texts) == len(nine_texts) == len(ten_texts)\n",
    "\n",
    "print (len(zero_texts))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def cleaning_function(input_texts):\n",
    "    k=0\n",
    "    input_texts_new=[]\n",
    "    for i in range(len(input_texts)):\n",
    "        k=k+1\n",
    "        matched = re.search('[0-9]+\\.txt:(.*)', input_texts[i])\n",
    "        try:\n",
    "            temp = matched.group(1)\n",
    "            # print (temp)\n",
    "            temp = re.sub('\\s+', ' ', temp)\n",
    "            temp = temp.strip()\n",
    "            input_texts_new.append(temp)\n",
    "        except:\n",
    "            print (\"ERROR\",input_texts[i])\n",
    "            # print (k)\n",
    "    return input_texts_new\n",
    "\n",
    "zero_texts=cleaning_function(zero_texts)\n",
    "one_texts=cleaning_function(one_texts)\n",
    "two_texts=cleaning_function(two_texts)\n",
    "three_texts=cleaning_function(three_texts)\n",
    "four_texts=cleaning_function(four_texts)\n",
    "five_texts=cleaning_function(five_texts)\n",
    "six_texts=cleaning_function(six_texts)\n",
    "seven_texts=cleaning_function(seven_texts)\n",
    "eight_texts=cleaning_function(eight_texts)\n",
    "nine_texts=cleaning_function(nine_texts)\n",
    "ten_texts=cleaning_function(ten_texts)\n",
    "\n",
    "assert len(zero_texts) == len(one_texts) == len(two_texts) == len(three_texts) == len(four_texts) == len(five_texts) == len(six_texts) == len(seven_texts) == len(eight_texts) == len(nine_texts) == len(ten_texts)\n",
    "\n",
    "\n",
    "print (len(zero_texts))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Several restaurants and bars are also located in The Octagon, and in fine weather there is a considerable amount of open - air dining.\n",
      "Several restaurants and bars are also located in The you you know Octagon, and in fine weather there is a considerable amount of open - air dining.\n",
      "Several restaurants and bars are also located in The Octagon, Octagon, Octagon, well I mean and in fine weather there is a considerable amount of open - air dining.\n",
      "Several restaurants and bars are also located in The The The The Octagon, so okay like and in fine weather there is a considerable amount of open - air dining.\n",
      "Several restaurants and bars are also located in The The The The The you know uh so well Octagon, and in fine weather there is a considerable amount of open - air dining.\n",
      "Several restaurants several restaurants several restaurants several restaurants several several several several several several restaurants several restaurants and bars are also located in The The The The The The so okay um I mean well Octagon, and in in in in in in fine weather there is a considerable amount of open - air dining.\n",
      "Several restaurants and bars are also located in The The The The The The The Octagon, and well uh so you know well I mean in in in in in in in fine weather there is a considerable amount of open - air dining.\n",
      "Several restaurants and bars are also located in The The The The The The The The Octagon, you know um you know okay like you you you you you you you you know um and in fine weather there is a considerable amount of open - air dining.\n",
      "Several restaurants and bars are also located you know know know know know know know know know uh you know okay um you know I mean mean mean mean mean mean mean mean mean okay in The Octagon, and in fine weather there is a considerable amount of open - air dining.\n",
      "Several restaurants and bars are also located in The The The The The The The The The The so um you know well okay so you know uh uh uh uh uh uh uh uh uh uh like Octagon, and in fine weather there is a considerable amount of open - air dining.\n",
      "Several restaurants and bars are also located in The Octagon, and and and and and and and and and and and well well like you know um well you know um um um um um um um um um um um like um in fine weather there is a considerable amount of open - air dining.\n"
     ]
    }
   ],
   "source": [
    "num_4 =10\n",
    "print (zero_texts[num_4])\n",
    "print (one_texts[num_4])\n",
    "print (two_texts[num_4])\n",
    "print (three_texts[num_4])\n",
    "print (four_texts[num_4])\n",
    "print (five_texts[num_4])\n",
    "print (six_texts[num_4])\n",
    "print (seven_texts[num_4])\n",
    "print (eight_texts[num_4])\n",
    "print (nine_texts[num_4])\n",
    "print (ten_texts[num_4])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5000\n"
     ]
    }
   ],
   "source": [
    "zero_texts_extended = [i.strip() for i in zero_texts]\n",
    "one_texts_extended = [i.strip() for i in one_texts]\n",
    "two_texts_extended = [i.strip() for i in two_texts]\n",
    "three_texts_extended = [i.strip() for i in three_texts]\n",
    "four_texts_extended = [i.strip() for i in four_texts]\n",
    "five_texts_extended = [i.strip() for i in five_texts]\n",
    "six_texts_extended = [i.strip() for i in six_texts]\n",
    "seven_texts_extended = [i.strip() for i in seven_texts]\n",
    "eight_texts_extended = [i.strip() for i in eight_texts]\n",
    "nine_texts_extended = [i.strip() for i in nine_texts]\n",
    "ten_texts_extended = [i.strip() for i in ten_texts]\n",
    "\n",
    "print (len(zero_texts_extended))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "no of errors 756\n",
      "4244\n",
      "Minimum length 95 100 109 109 95 95 95 111 109 95 111\n",
      "Maximum length 331 363 421 514 709 676 789 993 1015 1465 1267\n"
     ]
    }
   ],
   "source": [
    "zero_texts_final=[]\n",
    "one_texts_final=[]\n",
    "two_texts_final=[]\n",
    "three_texts_final=[]\n",
    "four_texts_final=[]\n",
    "five_texts_final=[]\n",
    "six_texts_final=[]\n",
    "seven_texts_final=[]\n",
    "eight_texts_final=[]\n",
    "nine_texts_final=[]\n",
    "ten_texts_final=[]\n",
    "\n",
    "\n",
    "def remove_unwanted_quotes(s):\n",
    "    return re.sub(r\"(^|[^a-zA-Z])'|'([^a-zA-Z]|$)\", r'\\1\\2', s)\n",
    "\n",
    "\n",
    "def to_lowercase(match):\n",
    "    return match.group(0).lower()\n",
    "\n",
    "# result = re.sub(r'\\b(?!(?:I\\b|I\\'[a-zA-Z]+|[A-Z]{2,}|[a-zA-Z])\\b)[a-zA-Z]+\\b', to_lowercase, text)\n",
    "\n",
    "\n",
    "word_min = 0\n",
    "\n",
    "def remove_non_english_chars(text):\n",
    "    # This regex retains English letters, digits, common punctuation, and whitespace.\n",
    "    # Any character not in this set will be replaced with an empty string.\n",
    "    pattern = r\"[^a-zA-Z0-9\\s.,!?@#$%&*()-_+=<>\\\"']\"\n",
    "    return re.sub(pattern, '', text)\n",
    "\n",
    "def remove_non_letters_from_start(sentence):\n",
    "    # Find the index where the first letter is\n",
    "    try:\n",
    "        sentence = remove_non_english_chars(sentence)\n",
    "        sentence = remove_unwanted_quotes(sentence).strip()\n",
    "        for idx, char in enumerate(sentence):\n",
    "            if char.isalpha():\n",
    "                break\n",
    "        if idx!=0:\n",
    "            print (\"Error\",sentence)\n",
    "        # Return the sentence starting from the first letter\n",
    "        text = sentence[idx:]\n",
    "        text = re.sub('[.,:;`!()%#-\\/]', '', text)\n",
    "        text = re.sub(\"''\", \"\", text)\n",
    "        \n",
    "        # Remove partial words followed by spaces or '.'\n",
    "        text = re.sub(r'\\b\\w+-([ .]|$)', '', text)\n",
    "        text = re.sub(r' +',' ', text)\n",
    "        text = re.sub(r'\\b(?!(?:I\\b|I\\'[a-zA-Z]+)\\b)[a-zA-Z]+\\b', to_lowercase, text.strip())\n",
    "    except:\n",
    "        text = sentence\n",
    "\n",
    "    return text\n",
    "\n",
    "\n",
    "\n",
    "k=0\n",
    "for a_0,a_1,a_2,a_3,a_4,a_5,a_6,a_7,a_8,a_9,a_10 in zip(zero_texts_extended,one_texts_extended,two_texts_extended,three_texts_extended,four_texts_extended,five_texts_extended,six_texts_extended,seven_texts_extended,eight_texts_extended,nine_texts_extended,ten_texts_extended):\n",
    "    if len(re.sub(\"\\s+\",\"\",a_0.strip()))>100:\n",
    "    #  remove_non_letters_from_start(a_0).strip()!=remove_non_letters_from_start(a_1).strip() and \\\n",
    "    #     remove_non_letters_from_start(a_0).strip()!=remove_non_letters_from_start(a_2).strip() and \\\n",
    "    #     remove_non_letters_from_start(a_0).strip()!=remove_non_letters_from_start(a_3).strip() and \\\n",
    "    #     remove_non_letters_from_start(a_0).strip()!=remove_non_letters_from_start(a_4).strip() and \\\n",
    "    #     remove_non_letters_from_start(a_0).strip()!=remove_non_letters_from_start(a_5).strip() and \\\n",
    "    #     remove_non_letters_from_start(a_0).strip()!=remove_non_letters_from_start(a_6).strip() and \\\n",
    "    #     remove_non_letters_from_start(a_0).strip()!=remove_non_letters_from_start(a_7).strip() and \\\n",
    "    #     remove_non_letters_from_start(a_0).strip()!=remove_non_letters_from_start(a_8).strip() and \\\n",
    "    #     remove_non_letters_from_start(a_0).strip()!=remove_non_letters_from_start(a_9).strip() and \\\n",
    "    #     remove_non_letters_from_start(a_0).strip()!=remove_non_letters_from_start(a_10).strip():\n",
    "\n",
    "\n",
    "        zero_texts_final.append(remove_non_letters_from_start(a_0))\n",
    "        one_texts_final.append(remove_non_letters_from_start(a_1))\n",
    "        two_texts_final.append(remove_non_letters_from_start(a_2))\n",
    "        three_texts_final.append(remove_non_letters_from_start(a_3))\n",
    "        four_texts_final.append(remove_non_letters_from_start(a_4))\n",
    "        five_texts_final.append(remove_non_letters_from_start(a_5))\n",
    "        six_texts_final.append(remove_non_letters_from_start(a_6))\n",
    "        seven_texts_final.append(remove_non_letters_from_start(a_7))\n",
    "        eight_texts_final.append(remove_non_letters_from_start(a_8))\n",
    "        nine_texts_final.append(remove_non_letters_from_start(a_9))\n",
    "        ten_texts_final.append(remove_non_letters_from_start(a_10))\n",
    "        \n",
    "\n",
    "    else:\n",
    "        k=k+1\n",
    "print (\"no of errors\",k)\n",
    "threshold = 50\n",
    "\n",
    "assert len(zero_texts_final)==len(one_texts_final)==len(two_texts_final)==len(three_texts_final)==len(four_texts_final)==len(five_texts_final)==len(six_texts_final)==len(seven_texts_final)==len(eight_texts_final)==len(nine_texts_final)==len(ten_texts_final)\n",
    "\n",
    "print (len(zero_texts_final))\n",
    "\n",
    "print (\"Minimum length\",min([len(i) for i in zero_texts_final]),min([len(i) for i in one_texts_final]),min([len(i) for i in two_texts_final]),min([len(i) for i in three_texts_final]),min([len(i) for i in four_texts_final]),min([len(i) for i in five_texts_final]),min([len(i) for i in six_texts_final]),min([len(i) for i in seven_texts_final]),min([len(i) for i in eight_texts_final]),min([len(i) for i in nine_texts_final]),min([len(i) for i in ten_texts_final]))\n",
    "print (\"Maximum length\",max([len(i) for i in zero_texts_final]),max([len(i) for i in one_texts_final]),max([len(i) for i in two_texts_final]),max([len(i) for i in three_texts_final]),max([len(i) for i in four_texts_final]),max([len(i) for i in five_texts_final]),max([len(i) for i in six_texts_final]),max([len(i) for i in seven_texts_final]),max([len(i) for i in eight_texts_final]),max([len(i) for i in nine_texts_final]),max([len(i) for i in ten_texts_final]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "riverside south memphis is one of the oldest portions of south memphis tennessee made up of mainly elderly people as well as young people\n",
      "riverside south memphis is one of the oldest portions you you know of south memphis tennessee made up of mainly elderly people as well as young people\n",
      "riverside south memphis is one of the oldest portions like like like like of south memphis tennessee made up of mainly elderly people as well as young people\n",
      "riverside south riverside south riverside south riverside south memphis is one one one one of the oldest portions of south well so I I I I mean memphis tennessee made up of mainly elderly people as well as young people\n",
      "riverside south riverside south riverside south riverside south riverside south south south south south memphis is one of the oldest portions I mean I mean mean mean mean mean um um of south memphis tennessee made made made made made up of mainly elderly people as well as young people\n",
      "riverside south memphis is one of the oldest portions of south south south south south south okay like uh okay uh memphis tennessee made up of mainly elderly people as well as young people\n",
      "riverside south memphis is one of the oldest portions of of of of of of of you know like I mean okay okay uh south memphis memphis memphis memphis memphis memphis memphis tennessee made up of mainly elderly people as well as young people\n",
      "riverside south memphis is one of the oldest oldest oldest oldest oldest oldest oldest oldest I mean I mean like uh uh I mean okay okay okay okay okay okay okay okay portions of south memphis tennessee made up of mainly elderly people as well as young people\n",
      "riverside south memphis is one of the oldest portions like well well well well well well well well well like I mean um well well you know of of of of of of of of of south memphis tennessee made up of mainly elderly people as well as young people\n",
      "riverside south riverside south riverside south riverside south riverside riverside riverside riverside riverside riverside riverside riverside riverside riverside south riverside south riverside south riverside south riverside south riverside south south south south south south south south south south memphis is one of the oldest portions I mean you you you you you you you you you you know like um um like I mean like I mean mean mean mean mean mean mean mean mean mean of south memphis tennessee made up of mainly elderly people as well as young people\n",
      "riverside south memphis is one of the oldest portions of of of of of of of of of of of uh um I mean like so so so I mean mean mean mean mean mean mean mean mean mean mean well so south memphis tennessee made up of mainly elderly people as well as young people\n",
      "-------------\n",
      "wendy schaal born july 2 1954 is an american actress and voice actress who is best known as the voice of francine smith in the tv series american dad\n",
      "wendy schaal born july 2 1954 is an american actress actress and you know voice actress who is best known as as the voice of francine smith in the tv series american dad\n",
      "wendy schaal born july 2 1954 is an american I I I mean so actress and voice actress who is best best best known as um you know the voice of francine smith in the tv series american dad\n",
      "wendy schaal born july 2 1954 is an american actress actress actress actress well you know um and voice actress who is is is is best known as the voice of francine smith in the tv series american dad\n",
      "wendy schaal born july 2 1954 is an an an an an american actress and so um uh uh voice actress who who who who who is best known as the voice of francine smith in the tv series american dad\n",
      "wendy schaal born july 2 1954 is an american actress uh uh uh uh uh uh uh I mean um like and voice actress actress actress actress actress actress who is best known as the voice of francine smith in the tv series american dad\n",
      "wendy schaal wendy schaal wendy schaal wendy schaal wendy schaal wendy wendy wendy wendy wendy wendy wendy schaal wendy schaal born july 2 1954 is an american american american american american american american actress well like um you know um um and and and and and and and voice actress who is best known as the the the the the the the voice you know um um so like so of francine smith in the tv series american dad\n",
      "wendy schaal born july 2 1954 is an american american american american american american american american you know so like so I mean I I I I I I I I mean you know actress and voice actress who is is is is is is is is best known as the voice of francine smith in the tv series american dad\n",
      "wendy schaal born july 2 1954 is an um you know know know know know know know know know you know okay okay okay okay uh american actress and and and and and and and and and voice actress who is best known as the the the the the the the the the well well you know like like I mean okay okay okay okay okay okay okay okay okay uh voice of francine smith in the tv series american dad\n",
      "wendy schaal wendy schaal wendy schaal wendy schaal wendy schaal schaal schaal schaal schaal schaal schaal schaal schaal schaal wendy schaal wendy schaal wendy schaal wendy schaal wendy wendy wendy wendy wendy wendy wendy wendy wendy wendy schaal born july 2 1954 is an american actress and like you you you you you you you you you you know okay okay um you know okay so uh voice voice voice voice voice voice voice voice voice voice actress who is best known as the voice of francine smith in the tv series american dad\n",
      "wendy schaal born july 2 1954 is an american actress actress actress actress actress actress actress actress actress actress actress and okay um so okay like well you know know know know know know know know know know know I mean you know okay voice actress who is best best best best best best best best best best best known as the voice of francine smith in the tv series american dad\n",
      "-------------\n",
      "in 2001 weston williamson developed and built their offices on tower bridge road london where they have an office of 60 architects working on a variety of projects\n",
      "in 2001 weston williamson developed and built their their offices so on tower bridge road london where where they have an you know office of 60 architects working on a variety of projects\n",
      "in 2001 weston williamson developed and built their offices well well well okay on tower bridge road london where they have an office of 60 architects working on a variety of projects\n",
      "in 2001 weston williamson developed and built their their their their offices on like well um tower bridge road london london london london where they have an office of 60 architects working on a variety of projects\n",
      "in 2001 weston williamson developed and built their uh uh uh uh uh uh so uh offices on tower bridge road london london london london london where they have an office of 60 architects working on a variety of projects\n",
      "in 2001 in 2001 in 2001 in 2001 in 2001 in in in in in in 2001 weston williamson developed and built their offices offices offices offices offices offices on well uh um like you know tower bridge road london london london london london london where they have an office of 60 architects working on a variety of projects\n",
      "in 2001 weston williamson developed and built their so so so so so so so you know like uh like you know offices on tower tower tower tower tower tower tower bridge road london where they have I mean I I I I I I I mean you know you know you know uh an office of 60 architects working on a variety of projects\n",
      "in 2001 weston williamson developed and built their their their their their their their their offices so I mean like okay well I mean okay okay okay okay okay okay okay okay on tower bridge road london where they have an office of 60 architects working on a variety of projects\n",
      "in 2001 weston williamson developed and built built built built built built built built built their offices so I mean I mean you know know know know know know know know know I mean well so I mean on tower bridge bridge bridge bridge bridge bridge bridge bridge bridge road london where they have an um I mean well well well well well well well well well um so I mean um you know office of 60 architects working on a variety of projects\n",
      "in 2001 weston williamson developed and built their offices offices offices offices offices offices offices offices offices offices on uh like so you know you know okay you you you you you you you you you you know um well tower bridge road london where they have an an an an an an an an an an office okay well you know okay okay um I mean mean mean mean mean mean mean mean mean mean okay you know of 60 architects working on a variety of projects\n",
      "in 2001 weston williamson developed and built their offices offices offices offices offices offices offices offices offices offices offices well um uh okay okay you know like so so so so so so so so so so so um like on tower bridge road london where they they they they they they they they they they they have an okay um well I mean I mean uh uh uh uh uh uh uh uh uh uh uh okay like so so office of 60 architects working on a variety of projects\n",
      "50\n",
      "61\n",
      "75\n",
      "86\n",
      "106\n",
      "126\n",
      "140\n",
      "175\n",
      "204\n",
      "269\n",
      "253\n",
      "15\n",
      "15\n",
      "15\n",
      "15\n",
      "15\n",
      "15\n",
      "15\n",
      "15\n",
      "15\n",
      "15\n",
      "15\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/grads/r/rohan.chaudhury/miniconda3/envs/multidoc2dial/lib/python3.9/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "You are using the default legacy behaviour of the <class 'transformers.models.t5.tokenization_t5.T5Tokenizer'>. If you see this, DO NOT PANIC! This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=True`. This should only be set if you understand what it means, and thouroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Maxinum_0\n",
      "72\n",
      "Maxinum_1\n",
      "66\n",
      "Maxinum_2\n",
      "103\n",
      "Maxinum_3\n",
      "127\n",
      "Maxinum_4\n",
      "132\n",
      "Maxinum_5\n",
      "134\n",
      "Maxinum_6\n",
      "161\n",
      "Maxinum_7\n",
      "253\n",
      "Maxinum_8\n",
      "243\n",
      "Maxinum_9\n",
      "302\n",
      "Maxinum_10\n",
      "277\n",
      "Maxinum_0\n",
      "57\n",
      "Maxinum_1\n",
      "70\n",
      "Maxinum_2\n",
      "109\n",
      "Maxinum_3\n",
      "109\n",
      "Maxinum_4\n",
      "112\n",
      "Maxinum_5\n",
      "166\n",
      "Maxinum_6\n",
      "249\n",
      "Maxinum_7\n",
      "253\n",
      "Maxinum_8\n",
      "284\n",
      "Maxinum_9\n",
      "302\n",
      "Maxinum_10\n",
      "393\n",
      "haryanvi devanagari also also known as bangru is the northernmost dialect of the hindi language\n",
      "haryanvi devanagari also also known as bangru like is the northernmost dialect of the hindi language\n",
      "in 1915 he graduated from lincoln university located in pennsylvania with a bachelors degree guthrie r v 1998\n",
      "in 1915 he graduated from lincoln university located in pennsylvania with a bachelors degree guthrie r v 1998\n",
      "haryanvi devanagari also also known as bangru is the northernmost dialect of the hindi language\n",
      "haryanvi devanagari also also known as bangru is the northernmost dialect of the hindi language\n",
      "haryanvi devanagari also also known as bangru is the northernmost dialect of the hindi language\n",
      "this results in the formation of glutamate semialdehyde which spontaneously cyclizes to pyrroline 5 carboxylate\n",
      "in 1915 he graduated from lincoln university located in pennsylvania with a bachelors degree guthrie r v 1998\n",
      "haryanvi devanagari also also known as bangru is the northernmost dialect of the hindi language\n",
      "this results in the formation of glutamate semialdehyde which spontaneously cyclizes to pyrroline 5 carboxylate\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# assert 1==2\n",
    "num_1 = 100\n",
    "num_2 = 50\n",
    "num_3 = 125\n",
    "\n",
    "print (zero_texts_final[num_3])\n",
    "print (one_texts_final[num_3])\n",
    "print (two_texts_final[num_3])\n",
    "print (three_texts_final[num_3])\n",
    "print (four_texts_final[num_3])\n",
    "print (five_texts_final[num_3])\n",
    "print (six_texts_final[num_3])\n",
    "print (seven_texts_final[num_3])\n",
    "print (eight_texts_final[num_3])\n",
    "print (nine_texts_final[num_3])\n",
    "print (ten_texts_final[num_3])\n",
    "\n",
    "\n",
    "\n",
    "print (\"-------------\")\n",
    " \n",
    "print (zero_texts_final[-1])\n",
    "print (one_texts_final[-1])\n",
    "print (two_texts_final[-1])\n",
    "print (three_texts_final[-1])\n",
    "print (four_texts_final[-1])\n",
    "print (five_texts_final[-1])\n",
    "print (six_texts_final[-1])\n",
    "print (seven_texts_final[-1])\n",
    "print (eight_texts_final[-1])\n",
    "print (nine_texts_final[-1])\n",
    "print (ten_texts_final[-1])\n",
    "\n",
    "\n",
    "\n",
    "print (\"-------------\")\n",
    "\n",
    "print (zero_texts_final[num_1])\n",
    "print (one_texts_final[num_1])\n",
    "print (two_texts_final[num_1])\n",
    "print (three_texts_final[num_1])\n",
    "print (four_texts_final[num_1])\n",
    "print (five_texts_final[num_1])\n",
    "print (six_texts_final[num_1])\n",
    "print (seven_texts_final[num_1])\n",
    "print (eight_texts_final[num_1])\n",
    "print (nine_texts_final[num_1])\n",
    "print (ten_texts_final[num_1])\n",
    "\n",
    "\n",
    "        \n",
    "print (max([len(i.split(\" \")) for i in zero_texts_final]))\n",
    "print (max([len(i.split(\" \")) for i in one_texts_final]))\n",
    "print (max([len(i.split(\" \")) for i in two_texts_final]))\n",
    "print (max([len(i.split(\" \")) for i in three_texts_final]))\n",
    "print (max([len(i.split(\" \")) for i in four_texts_final]))\n",
    "print (max([len(i.split(\" \")) for i in five_texts_final]))\n",
    "print (max([len(i.split(\" \")) for i in six_texts_final]))\n",
    "print (max([len(i.split(\" \")) for i in seven_texts_final]))\n",
    "print (max([len(i.split(\" \")) for i in eight_texts_final]))\n",
    "print (max([len(i.split(\" \")) for i in nine_texts_final]))\n",
    "print (max([len(i.split(\" \")) for i in ten_texts_final]))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "print (min([len(i.split(\" \")) for i in zero_texts_final]))\n",
    "print (min([len(i.split(\" \")) for i in one_texts_final]))\n",
    "print (min([len(i.split(\" \")) for i in two_texts_final]))\n",
    "print (min([len(i.split(\" \")) for i in three_texts_final]))\n",
    "print (min([len(i.split(\" \")) for i in four_texts_final]))\n",
    "print (min([len(i.split(\" \")) for i in five_texts_final]))\n",
    "print (min([len(i.split(\" \")) for i in six_texts_final]))\n",
    "print (min([len(i.split(\" \")) for i in seven_texts_final]))\n",
    "print (min([len(i.split(\" \")) for i in eight_texts_final]))\n",
    "print (min([len(i.split(\" \")) for i in nine_texts_final]))\n",
    "print (min([len(i.split(\" \")) for i in ten_texts_final]))\n",
    "\n",
    "\n",
    "\n",
    "import numpy as np\n",
    "from numpy import argmin\n",
    "from numpy import argmax\n",
    "\n",
    "\n",
    "maximum_0 =zero_texts_final[argmax([len(i) for i in zero_texts_final])]\n",
    "maximum_1= one_texts_final[argmax([len(i) for i in one_texts_final])]\n",
    "maximum_2= two_texts_final[argmax([len(i) for i in two_texts_final])]\n",
    "maximum_3= three_texts_final[argmax([len(i) for i in three_texts_final])]\n",
    "maximum_4= four_texts_final[argmax([len(i) for i in four_texts_final])]\n",
    "maximum_5= five_texts_final[argmax([len(i) for i in five_texts_final])]\n",
    "maximum_6= six_texts_final[argmax([len(i) for i in six_texts_final])]\n",
    "maximum_7= seven_texts_final[argmax([len(i) for i in seven_texts_final])]\n",
    "maximum_8= eight_texts_final[argmax([len(i) for i in eight_texts_final])]\n",
    "maximum_9= nine_texts_final[argmax([len(i) for i in nine_texts_final])]\n",
    "maximum_10= ten_texts_final[argmax([len(i) for i in ten_texts_final])]\n",
    "\n",
    "from transformers import T5Tokenizer\n",
    "#calculate the number of tokens from T5 tokenizer\n",
    "t5_pretrained_model_name = \"t5-base\"\n",
    "model_max_length = 1024\n",
    "t5_tokenizer = T5Tokenizer.from_pretrained(t5_pretrained_model_name, model_max_length=model_max_length)\n",
    "\n",
    "print (\"Maxinum_0\")\n",
    "question_tokens = t5_tokenizer(maximum_0, truncation=False, return_tensors=\"pt\", padding=False)\n",
    "print(len(question_tokens[\"input_ids\"].squeeze()))\n",
    "print (\"Maxinum_1\")\n",
    "question_tokens = t5_tokenizer(maximum_1, truncation=False, return_tensors=\"pt\", padding=False)\n",
    "print(len(question_tokens[\"input_ids\"].squeeze()))\n",
    "print (\"Maxinum_2\")\n",
    "question_tokens = t5_tokenizer(maximum_2, truncation=False, return_tensors=\"pt\", padding=False)\n",
    "print(len(question_tokens[\"input_ids\"].squeeze()))\n",
    "print (\"Maxinum_3\")\n",
    "question_tokens = t5_tokenizer(maximum_3, truncation=False, return_tensors=\"pt\", padding=False)\n",
    "print(len(question_tokens[\"input_ids\"].squeeze()))\n",
    "print (\"Maxinum_4\")\n",
    "question_tokens = t5_tokenizer(maximum_4, truncation=False, return_tensors=\"pt\", padding=False)\n",
    "print(len(question_tokens[\"input_ids\"].squeeze()))\n",
    "print (\"Maxinum_5\")\n",
    "question_tokens = t5_tokenizer(maximum_5, truncation=False, return_tensors=\"pt\", padding=False)\n",
    "print(len(question_tokens[\"input_ids\"].squeeze()))\n",
    "print (\"Maxinum_6\")\n",
    "question_tokens = t5_tokenizer(maximum_6, truncation=False, return_tensors=\"pt\", padding=False)\n",
    "print(len(question_tokens[\"input_ids\"].squeeze()))\n",
    "print (\"Maxinum_7\")\n",
    "question_tokens = t5_tokenizer(maximum_7, truncation=False, return_tensors=\"pt\", padding=False)\n",
    "print(len(question_tokens[\"input_ids\"].squeeze()))\n",
    "print (\"Maxinum_8\")\n",
    "question_tokens = t5_tokenizer(maximum_8, truncation=False, return_tensors=\"pt\", padding=False)\n",
    "print(len(question_tokens[\"input_ids\"].squeeze()))\n",
    "print (\"Maxinum_9\")\n",
    "question_tokens = t5_tokenizer(maximum_9, truncation=False, return_tensors=\"pt\", padding=False)\n",
    "print(len(question_tokens[\"input_ids\"].squeeze()))\n",
    "print (\"Maxinum_10\")\n",
    "question_tokens = t5_tokenizer(maximum_10, truncation=False, return_tensors=\"pt\", padding=False)\n",
    "print(len(question_tokens[\"input_ids\"].squeeze()))\n",
    "\n",
    "\n",
    "maximum_0 =zero_texts_final[argmax([len(i.split(\" \")) for i in zero_texts_final])]\n",
    "maximum_1= one_texts_final[argmax([len(i.split(\" \")) for i in one_texts_final])]\n",
    "maximum_2= two_texts_final[argmax([len(i.split(\" \")) for i in two_texts_final])]\n",
    "maximum_3= three_texts_final[argmax([len(i.split(\" \")) for i in three_texts_final])]\n",
    "maximum_4= four_texts_final[argmax([len(i.split(\" \")) for i in four_texts_final])]\n",
    "maximum_5= five_texts_final[argmax([len(i.split(\" \")) for i in five_texts_final])]\n",
    "maximum_6= six_texts_final[argmax([len(i.split(\" \")) for i in six_texts_final])]\n",
    "maximum_7= seven_texts_final[argmax([len(i.split(\" \")) for i in seven_texts_final])]\n",
    "maximum_8= eight_texts_final[argmax([len(i.split(\" \")) for i in eight_texts_final])]\n",
    "maximum_9= nine_texts_final[argmax([len(i.split(\" \")) for i in nine_texts_final])]\n",
    "maximum_10= ten_texts_final[argmax([len(i.split(\" \")) for i in ten_texts_final])]\n",
    "\n",
    "from transformers import T5Tokenizer\n",
    "#calculate the number of tokens from T5 tokenizer\n",
    "t5_pretrained_model_name = \"t5-base\"\n",
    "model_max_length = 1024\n",
    "t5_tokenizer = T5Tokenizer.from_pretrained(t5_pretrained_model_name, model_max_length=model_max_length)\n",
    "\n",
    "\n",
    "print (\"Maxinum_0\")\n",
    "question_tokens = t5_tokenizer(maximum_0, truncation=False, return_tensors=\"pt\", padding=False)\n",
    "print(len(question_tokens[\"input_ids\"].squeeze()))\n",
    "print (\"Maxinum_1\")\n",
    "question_tokens = t5_tokenizer(maximum_1, truncation=False, return_tensors=\"pt\", padding=False)\n",
    "print(len(question_tokens[\"input_ids\"].squeeze()))\n",
    "print (\"Maxinum_2\")\n",
    "question_tokens = t5_tokenizer(maximum_2, truncation=False, return_tensors=\"pt\", padding=False)\n",
    "print(len(question_tokens[\"input_ids\"].squeeze()))\n",
    "print (\"Maxinum_3\")\n",
    "question_tokens = t5_tokenizer(maximum_3, truncation=False, return_tensors=\"pt\", padding=False)\n",
    "print(len(question_tokens[\"input_ids\"].squeeze()))\n",
    "print (\"Maxinum_4\")\n",
    "question_tokens = t5_tokenizer(maximum_4, truncation=False, return_tensors=\"pt\", padding=False)\n",
    "print(len(question_tokens[\"input_ids\"].squeeze()))\n",
    "print (\"Maxinum_5\")\n",
    "question_tokens = t5_tokenizer(maximum_5, truncation=False, return_tensors=\"pt\", padding=False)\n",
    "print(len(question_tokens[\"input_ids\"].squeeze()))\n",
    "print (\"Maxinum_6\")\n",
    "question_tokens = t5_tokenizer(maximum_6, truncation=False, return_tensors=\"pt\", padding=False)\n",
    "print(len(question_tokens[\"input_ids\"].squeeze()))\n",
    "print (\"Maxinum_7\")\n",
    "question_tokens = t5_tokenizer(maximum_7, truncation=False, return_tensors=\"pt\", padding=False)\n",
    "print(len(question_tokens[\"input_ids\"].squeeze()))\n",
    "print (\"Maxinum_8\")\n",
    "question_tokens = t5_tokenizer(maximum_8, truncation=False, return_tensors=\"pt\", padding=False)\n",
    "print(len(question_tokens[\"input_ids\"].squeeze()))\n",
    "print (\"Maxinum_9\")\n",
    "question_tokens = t5_tokenizer(maximum_9, truncation=False, return_tensors=\"pt\", padding=False)\n",
    "print(len(question_tokens[\"input_ids\"].squeeze()))\n",
    "print (\"Maxinum_10\")\n",
    "question_tokens = t5_tokenizer(maximum_10, truncation=False, return_tensors=\"pt\", padding=False)\n",
    "print(len(question_tokens[\"input_ids\"].squeeze()))\n",
    "\n",
    "\n",
    "\n",
    "print (zero_texts_final[argmin([len(i) for i in zero_texts_final])])\n",
    "print (one_texts_final[argmin([len(i) for i in one_texts_final])])\n",
    "print (two_texts_final[argmin([len(i) for i in two_texts_final])])\n",
    "print (three_texts_final[argmin([len(i) for i in three_texts_final])])\n",
    "print (four_texts_final[argmin([len(i) for i in four_texts_final])])\n",
    "print (five_texts_final[argmin([len(i) for i in five_texts_final])])\n",
    "print (six_texts_final[argmin([len(i) for i in six_texts_final])])\n",
    "print (seven_texts_final[argmin([len(i) for i in seven_texts_final])])\n",
    "print (eight_texts_final[argmin([len(i) for i in eight_texts_final])])\n",
    "print (nine_texts_final[argmin([len(i) for i in nine_texts_final])])\n",
    "print (ten_texts_final[argmin([len(i) for i in ten_texts_final])])\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# print (argmin([len(i) for i in zero_texts_final]))\n",
    "# print (argmin([len(i) for i in one_texts_final]))\n",
    "\n",
    "\n",
    "directory_to_save = f\"./wikipedia/{type}/\"\n",
    "os.makedirs(directory_to_save, exist_ok=True)\n",
    "\n",
    "\n",
    "with open(directory_to_save+\"0.txt\", 'w') as f:\n",
    "    f.write('\\n'.join(zero_texts_final))\n",
    "with open(directory_to_save+\"1.txt\", 'w') as f:\n",
    "    f.write('\\n'.join(one_texts_final))\n",
    "with open(directory_to_save+\"2.txt\", 'w') as f:\n",
    "    f.write('\\n'.join(two_texts_final))\n",
    "with open(directory_to_save+\"3.txt\", 'w') as f:\n",
    "    f.write('\\n'.join(three_texts_final))\n",
    "with open(directory_to_save+\"4.txt\", 'w') as f:\n",
    "    f.write('\\n'.join(four_texts_final))\n",
    "with open(directory_to_save+\"5.txt\", 'w') as f:\n",
    "    f.write('\\n'.join(five_texts_final))\n",
    "with open(directory_to_save+\"6.txt\", 'w') as f:\n",
    "    f.write('\\n'.join(six_texts_final))\n",
    "with open(directory_to_save+\"7.txt\", 'w') as f:\n",
    "    f.write('\\n'.join(seven_texts_final))\n",
    "with open(directory_to_save+\"8.txt\", 'w') as f:\n",
    "    f.write('\\n'.join(eight_texts_final))\n",
    "with open(directory_to_save+\"9.txt\", 'w') as f:\n",
    "    f.write('\\n'.join(nine_texts_final))\n",
    "with open(directory_to_save+\"10.txt\", 'w') as f:\n",
    "    f.write('\\n'.join(ten_texts_final))\n",
    "\n",
    "print (\"Done\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "multidoc2dial",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
