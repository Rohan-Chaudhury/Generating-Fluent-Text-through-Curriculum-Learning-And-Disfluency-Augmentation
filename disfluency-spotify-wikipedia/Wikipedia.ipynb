{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "afd72125-5c06-4589-ae8d-eb6ca0fb9eef",
   "metadata": {},
   "source": [
    "# Wikipedia"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e6e4e0e7-64ba-48a9-9cd9-2715dccb5bcd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import regex as re\n",
    "import numpy as np\n",
    "import os\n",
    "import difflib\n",
    "\n",
    "import utils_general\n",
    "import utils_transformations\n",
    "\n",
    "f = \"/data2/maria/wiki-split/test.tsv\"\n",
    "\n",
    "df = pd.read_csv(f, sep=\"\\t\", header=None)\n",
    "\n",
    "WIKIPEDIA_DIR = os.path.join(\".\", \"wikipedia\")\n",
    "utils_general.create_and_or_clear_this_dir(WIKIPEDIA_DIR)\n",
    "\n",
    "rng = np.random.default_rng(seed=0)\n",
    "\n",
    "def write_wikipedia_files_out():\n",
    "    for i in range(len(df)):\n",
    "        sentence = df.iloc[i,0]\n",
    "        \n",
    "        # shift punctuation\n",
    "        sentence = re.sub(r\"\\s*,\\s*\", \", \", sentence)\n",
    "        sentence = re.sub(r\"\\s*\\.\\s*\", \". \", sentence)\n",
    "        sentence = re.sub(r\"\\s*\\;\\s*\", \"; \", sentence)\n",
    "        sentence = re.sub(r\"\\s*\\:\\s*\", \": \", sentence)\n",
    "        sentence = re.sub(r\"\\s*\\!\\s*\", \"! \", sentence)\n",
    "        sentence = re.sub(r\"\\s*\\?\\s*\", \"? \", sentence)\n",
    "        \n",
    "        # special case\n",
    "        sentence = sentence.replace(\" n't\",\"n't\")\n",
    "        \n",
    "        sentence = re.sub(r\"''\",\"'\", sentence)\n",
    "        sentence = re.sub(r\"'\\s*'\",\"'\", sentence)\n",
    "        sentence = re.sub(r\"' ([^']+?) '\", r\" '\\1'\", sentence)\n",
    "        sentence = re.sub(r\"\\( ([^']+?) \\)\", r\"(\\1)\", sentence)\n",
    "        sentence = re.sub(r\"\\b\\s'\", \"'\", sentence)\n",
    "        sentence = re.sub(r\"\\.\\s*\\.\\s*\\.\", \"...\", sentence)\n",
    "        sentence = re.sub(r\"\\s+\", \" \", sentence)\n",
    "        sentence = sentence.strip()\n",
    "\n",
    "        utils_general.write_file(str(i) + \".txt\", WIKIPEDIA_DIR, sentence)    \n",
    "\n",
    "        for t in [\"repeats\",\"interjections\",\"false-starts\",\"repeats-and-false-starts\",\"repeats-and-interjections\",\"interjections-and-false-starts\",\"all-3\"]:\n",
    "            current_trf_dir = os.path.join(\".\", \"wikipedia\", t)\n",
    "            utils_general.just_create_this_dir(current_trf_dir)\n",
    "\n",
    "            for n in [0,1,2,3,4,5,6,7,8,9,10]:\n",
    "\n",
    "                # get the new filename\n",
    "                new_filename = str(n) + \"_\" + str(i) + \".txt\"\n",
    "\n",
    "                if t == \"repeats\":\n",
    "                    transcript_text = utils_transformations.get_repeats_text(n, sentence, rng)\n",
    "\n",
    "                elif t == \"interjections\":\n",
    "                    transcript_text = utils_transformations.get_interjections_text(n, sentence, rng)  \n",
    "\n",
    "                elif t == \"false-starts\":\n",
    "                    transcript_text = utils_transformations.get_false_starts_text(n, sentence, rng)\n",
    "\n",
    "                elif t == \"repeats-and-false-starts\":\n",
    "                    transcript_text = utils_transformations.get_repeats_text(n, sentence, rng)\n",
    "                    transcript_text = utils_transformations.get_false_starts_text(n, sentence, rng)\n",
    "\n",
    "                elif t == \"repeats-and-interjections\":\n",
    "                    transcript_text = utils_transformations.get_repeats_text(n, sentence, rng)\n",
    "                    transcript_text = utils_transformations.get_interjections_text(n, sentence, rng) \n",
    "\n",
    "                elif t == \"interjections-and-false-starts\":\n",
    "                    transcript_text = utils_transformations.get_interjections_text(n, sentence, rng)\n",
    "                    transcript_text = utils_transformations.get_false_starts_text(n, sentence, rng)\n",
    "\n",
    "                elif t == \"all-3\":\n",
    "                    transcript_text = utils_transformations.get_interjections_text(n, sentence, rng)\n",
    "                    transcript_text = utils_transformations.get_false_starts_text(n, sentence, rng)\n",
    "                    transcript_text = utils_transformations.get_repeats_text(n, sentence, rng)  \n",
    "\n",
    "                utils_general.write_file(new_filename, current_trf_dir, sentence)\n",
    "                \n",
    "write_wikipedia_files_out()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "65b03c16-4c81-4f77-8d7c-9749c6843654",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'Bandolier - Budgie', a free iTunes app for iPad, iPhone and iPod touch, released in December 2011, tells the story of the making of Bandolier in the band's own words - including an extensive audio interview with Burke Shelley. \n",
      "\n",
      "'Eden Black' was grown from seed in the late 1980s by Stephen Morley, under his conditions it produces pitchers that are almost completley black. \n",
      "\n",
      "'Wilson should extend his stint on The Voice to renew public interest in the band; given that they're pulling out all the stops, they deserve all the acclaim that surrounded them for their first two albums. \n",
      "\n",
      "'New York Mining Disaster 1941' was the second EP released by the Bee Gees in 1967 on the Spin Records, like their first EP, it was released only in Australia. \n",
      "\n",
      "'ADAPTOGENS: Herbs for Strength, Stamina, and Stress Relief,' Healing Arts Press, 2007 - contains a detailed monograph on Schisandra chinensis as well as highlights health benefits. \n",
      "\n",
      "'Aerodynamic' is an instrumental song by Daft Punk that is particularly well - known for its robotic guitar solo. \n",
      "\n",
      "' After a lawsuit by Ray Lynch for allegedly not paying him, the company was foreclosed by Security Pacific Bank on November 1991 with all of its assets sold by June 1992. \n",
      "\n",
      "' Again Schwartz claims 100 % hits for Susy Smith, but nothing for his true grandmothers. \n",
      "\n",
      "'Bellringer' was in fact a derivative of 'Hellbringer,' a nickname given to him by fellow musician Dan Massie in reference to his unquenchable thirst for debauchery and outlandish clothing. \n",
      "\n",
      "'Chalayil mahavishnu kshetram' is a famous Vishnu temple situated in Mattanur is one of the rare temples for worshiping Narasimha avathara of lord Vishnu. \n",
      "\n",
      "'Day Is Done' is the fifth track from the album 'Five Leaves Left' by the British folk musician Nick Drake. \n",
      "\n",
      "'Eazy - Duz - It' is a single by late rapper Eazy - E, from his album of the same name, 'Eazy - Duz - It', that was released in 1989 and written by fellow N. W. A group mate, MC Ren. \n",
      "\n",
      "'Flying Easy Loving Crazy' is Toshinobu Kubota's thirty - third single, featuring R&B singer Misia, released on March 26, 2008. \n",
      "\n",
      "'For All Time' was formally released to radio on February 12, 2002. \n",
      "\n",
      "'Got You (Where I Want You)' was also featured on the soundtrack for the 1998 MGM film Disturbing Behavior, which starred Katie Holmes. There music is also featured in The Crow salvation soundtrack, with the track I Know What You Want. \n",
      "\n",
      "'Hotel California' topped the 'Billboard' Hot 100 singles chart for one week in May 1977 and peaked at number 10 on the Easy Listening chart. \n",
      "\n",
      "'I'm Alive 'is a song written by the American rock band Tommy James and the Shondells and first recorded in 1969 as part of their album' Crimson and Clover'. \n",
      "\n",
      "' I Don't Care if the Sun Don't Shine' is a popular song, popularized by Patti Page in 1950. \n",
      "\n",
      "'I Lost My Heart to a Starship Trooper', sometimes cited as '(I Lost My Heart to a) Starship Trooper', is the first single by Hot Gossip, notable as the debut of the then teenage Sarah Brightman as a singer. \n",
      "\n",
      "'Ice Cream of Margie (with the Light Blue Hair)' is the seventh episode of 'The Simpsons' eighteenth season, originally airing on the Fox network in the United States on November 26, 2006. \n",
      "\n",
      "'Kuka Kanyini' draws on traditional land management practices and sets out priorities for scientists to work with Indigenous communities to help them manage their lands themselves. \n",
      "\n",
      "'Layla' is a song written by Eric Clapton and Jim Gordon, originally released by their blues rock band Derek and the Dominos, as the thirteenth track from their album 'Layla and Other Assorted Love Songs' (November 1970). \n",
      "\n",
      "'Lord Snow' is the third episode of the HBO medieval fantasy television series 'Game of Thrones', first aired on May 1, 2011. \n",
      "\n",
      "'Lot of Leavin' Left to Do 'is the lead - off single to Dierks Bentley's 2005 album 'Modern Day Drifter'. \n",
      "\n",
      "'Massive Attack' is a song by Trinidadian recording artist Nicki Minaj and American singer - songwriter Sean Garrett, for Minaj's debut album 'Pink Friday'. \n",
      "\n",
      "'My Sweet Lord' is a song by English musician and former Beatle George Harrison that was released in November 1970 on his triple album 'All Things Must Pass'. \n",
      "\n",
      "'Nobody But Me' is a single by American country music singer Blake Shelton that reached the Top 5 on the 'Billboard' Hot Country Songs chart. \n",
      "\n",
      "'Oxford Dictionary of National Biography;' Abraham, Sir Edward Penley 'Shortly after the team published its first results in 1940, Fleming telephoned Howard Florey, Chain's head of department, to say that he would be visiting within the next few days. \n",
      "\n",
      "'Paper Plane' is a rock song originally by Status Quo, which was released as a single in 1972 (reaching number 8 in the UK Singles Chart) and on their album 'Piledriver'. \n",
      "\n",
      "'Queen of the Night' is a song by Whitney Houston, and was the fifth and final single released from the 'The Bodyguard' soundtrack album. \n",
      "\n"
     ]
    }
   ],
   "source": [
    "for i in range(0,30):\n",
    "    id_str = str(i)\n",
    "    print(utils_general.read_file(os.path.join(\".\", \"wikipedia\", f\"{id_str}.txt\")), \"\\n\")\n",
    "    # print(utils_general.read_file(os.path.join(\".\", \"wikipedia\", \"repeats\", f\"0_{id_str}.txt\")), \"\\n\")\n",
    "    # print(utils_general.read_file(os.path.join(\".\", \"wikipedia\", \"repeats\", f\"3_{id_str}.txt\")), \"\\n\")\n",
    "    # print(utils_general.read_file(os.path.join(\".\", \"wikipedia\", \"interjections\", f\"3_{id_str}.txt\")), \"\\n\")\n",
    "    # print(utils_general.read_file(os.path.join(\".\", \"wikipedia\", \"false-starts\", f\"3_{id_str}.txt\")), \"\\n\")\n",
    "    # print(utils_general.read_file(os.path.join(\".\", \"wikipedia\", \"all-3\", f\"3_{id_str}.txt\")), \"\\n\")\n",
    "\n",
    "# for id_str in range(0,1000):\n",
    "#     print(utils_general.read_file(os.path.join(\".\", \"wikipedia\", f\"{id_str}.txt\")), \"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46f13b75-230e-440a-9da2-8b452535a3c4",
   "metadata": {},
   "source": [
    "## Reproducibility Checks\n",
    "\n",
    "Makes sure that the output of these 3 files (selected randomly) is the same."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6677da56-7f57-482d-a26e-96e2076e668d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[]\n",
      "[]\n",
      "[]\n"
     ]
    }
   ],
   "source": [
    "random_file_1_path = os.path.join(\".\", \"wikipedia\", \"repeats-and-false-starts\",\"8_1.txt\")\n",
    "random_file_2_path = os.path.join(\".\", \"wikipedia\", \"interjections\",\"0_3908.txt\")\n",
    "random_file_3_path = os.path.join(\".\", \"wikipedia\", \"all-3\",\"0_432.txt\")\n",
    "\n",
    "# reads the output of 3 files\n",
    "random_file_1 = utils_general.read_file(random_file_1_path)\n",
    "random_file_2 = utils_general.read_file(random_file_2_path)\n",
    "random_file_3 = utils_general.read_file(random_file_3_path)\n",
    "\n",
    "# re-runs the writing files out & transformations\n",
    "write_wikipedia_files_out()\n",
    "run2_random_file_1 = utils_general.read_file(random_file_1_path)\n",
    "run2_random_file_2 = utils_general.read_file(random_file_2_path)\n",
    "run2_random_file_3 = utils_general.read_file(random_file_3_path)\n",
    "\n",
    "# ensures that the new files are the same as the old files\n",
    "print(list(difflib.unified_diff(random_file_1.split(\" \"), run2_random_file_1.split(\" \"))))\n",
    "print(list(difflib.unified_diff(random_file_2.split(\" \"), run2_random_file_2.split(\" \"))))\n",
    "print(list(difflib.unified_diff(random_file_3.split(\" \"), run2_random_file_3.split(\" \"))))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
